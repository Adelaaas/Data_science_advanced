{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rasyuk_languages.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvWs8-hMLtbj",
        "outputId": "9bc194c8-48e9-47d5-a883-ade2251d8d30"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAXLZZG4GEr3"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "url = os.chdir(r'gdrive/MyDrive/data')\n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbubJ9bXGGiv"
      },
      "source": [
        "files = os.listdir(url)\n",
        "files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUdjkASGMjqJ"
      },
      "source": [
        "k = ['train_da', 'train_bg', 'train_cs', 'train_de', 'train_el', 'train_es', 'train_et', 'train_fi',\n",
        "     'train_fr', 'train_hu', 'train_it', 'train_lt', 'train_lv', 'train_nl', 'train_pl', 'train_pt',\n",
        "     'train_ro', 'train_sk', 'train_sl', 'train_sv', 'test', 'test_submission']\n",
        "     \n",
        "lang = {'train_da':[], 'train_bg':[], 'train_cs':[], 'train_de':[], 'train_el':[], 'train_es':[], 'train_et':[],\n",
        "        'train_fi':[], 'train_fr':[], 'train_hu':[], 'train_it':[], 'train_lt':[], 'train_lv':[], 'train_nl':[],\n",
        "        'train_pl':[], 'train_pt':[], 'train_ro':[], 'train_sk':[], 'train_sl':[], 'train_sv':[], 'test':[], 'test_submission':[]}\n",
        "\n",
        "i = 0\n",
        "for file in files:\n",
        "    with open(file, \"r\", encoding='utf-8') as f:\n",
        "        for line in f.readlines():\n",
        "            lang[k[i]].append(line)\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KQtX4gzGH2v"
      },
      "source": [
        "# re - библиотека для работы с регулярными выражениями и строками\n",
        "for l in lang:\n",
        "    if re.findall('train', l):\n",
        "        k = l.split('train_')[1]\n",
        "        print(l, '|', k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDggLzcAGJzf"
      },
      "source": [
        "# создание тренировочного набора\n",
        "train = pd.DataFrame(columns=['text', 'language'])\n",
        "\n",
        "for l in lang:\n",
        "    if re.findall('train', l):\n",
        "        tmp = pd.DataFrame()\n",
        "        tmp['text'] = lang[l]\n",
        "        tmp['language'] = l.split('train_')[1]\n",
        "        train = pd.concat([train, tmp], ignore_index=True)\n",
        "\n",
        "print(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8MVkIIVGLxH"
      },
      "source": [
        "print(f'What languages are represented in the data: {train[\"language\"].unique()}')\n",
        "print('-'*100)\n",
        "print(f'How many sentences are presented for each language: \\n{train[\"language\"].value_counts()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FNvMt_NGNYP"
      },
      "source": [
        "test = pd.DataFrame(columns=['text'])\n",
        "tmp = []\n",
        "\n",
        "for word in lang['test']:\n",
        "    tmp.append(word.split(',')[1].split('\\n')[0])\n",
        "\n",
        "test['text'] = tmp\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGFs__8qGP0B"
      },
      "source": [
        "word = []\n",
        "lang = []\n",
        "for i in range(300000):\n",
        "  p = '''!()-[]{};?@#$%:'\"\\,./^&;*_0123456789'''\n",
        "  s = (train['text'][i])\n",
        "  new = ''\n",
        "  for e in s:\n",
        "    if e not in p:\n",
        "      new += e  \n",
        "  k = new.split()\n",
        "  for j in range(len(k)):\n",
        "    word.append(k[j])\n",
        "    lang.append(train['language'][i])\n",
        "\n",
        "train_2 = pd.DataFrame({'word': word, 'lang': lang})\n",
        "train_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KS7nPYzLGSZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3605058b-7135-4f46-965f-67861d133736"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 8.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlksyKwJGRRi"
      },
      "source": [
        "import pymorphy2\n",
        "\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "lm_txt = []\n",
        "for i in range(len(train)):\n",
        "  lm_wd = []\n",
        "  words = train.loc[i]['Question'].split(' ')\n",
        "  for i in words:\n",
        "      lm = morph.parse(i)\n",
        "      lm_wd.append(lm[0].normal_form)\n",
        "  text = ' '.join(lm_wd)\n",
        "  lm_txt.append(text)\n",
        "\n",
        "train['Question'] = lm_txt\n",
        "\n",
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KhEJuIFGX_n"
      },
      "source": [
        "lm_txt = []\n",
        "for i in range(len(test)):\n",
        "  lm_wd = []\n",
        "  words = test.loc[i]['Question'].split(' ')\n",
        "  for i in words:\n",
        "      lm = morph.parse(i)\n",
        "      lm_wd.append(lm[0].normal_form)\n",
        "  text = ' '.join(lm_wd)\n",
        "  lm_txt.append(text)\n",
        "\n",
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0vvt7KTGdxn"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_2['word'], train_2['lang'], test_size=0.2, random_state=42)\n",
        "\n",
        "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
        "X_test_counts = count_vectorizer.transform(X_test)\n",
        "test_counts = count_vectorizer.transform(test[\"text\"])\n",
        "\n",
        "\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(X_train_counts, y_train)\n",
        "\n",
        "y_predicted_counts = clf.predict(X_test_counts)\n",
        "\n",
        "y_predicted_test = clf.predict(test_counts)\n",
        "y_test_pred = pd.DataFrame(y_predicted_test, columns=['text'])\n",
        "y_test_pred.reset_index(inplace=True)\n",
        "\n",
        "y_test_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQCP4NqxGrj5"
      },
      "source": [
        "y_test_pred.to_csv('submition_test_Rasyuk.csv', index=False)\n",
        "files.download(\"submition_test_Rasyuk.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}